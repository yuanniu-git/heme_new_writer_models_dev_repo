{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb7944c-e4c3-4839-93fd-8f1022a025e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Training binary classification model for Jivi restart writers\n",
    "\n",
    "## TODO: Hyper-parameter tuning. Further champion model hunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb8e736-30ce-4eaf-b8e9-eeee60a68ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e44c0d-119f-4310-ae0a-729946c0a1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "563d5487-0e02-48ad-b4d8-4430c5d76991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mlflow\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, \n",
    "    f1_score, precision_recall_curve, auc, confusion_matrix, classification_report\n",
    ")\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29cf81b8-0210-4341-8583-eb34f215a918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../00_config/set-up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "287c301a-8239-4b62-a664-76bcbe38cfda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Month and Date parameters for manual control\n",
    "first_month = \"2019-12\"\n",
    "last_month = \"2024-11\"\n",
    "\n",
    "train_start_month = \"2023-01\"\n",
    "train_end_month = \"2024-04\"\n",
    "test_start_month = \"2024-05\"\n",
    "test_end_month = \"2024-11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0bbca16-6d71-49c9-a460-e10cd2767ec4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the feature master table from Hivestore\n",
    "hcp_feats_master_w_target_sdf = spark.sql(\"SELECT * FROM jivi_new_writer_model.hcp_feats_master_w_target\")\n",
    "print(\n",
    "    \"Row count: \",\n",
    "    hcp_feats_master_w_target_sdf.count(),\n",
    "    \"Column Count: \",\n",
    "    len(hcp_feats_master_w_target_sdf.columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbded0fe-6ffb-4101-b743-596ade038d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converting Spark dataframe to Pandas dataframe\n",
    "hcp_feats_master_w_target_pdf = hcp_feats_master_w_target_sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ca94fc-1c5f-400c-83a8-9a9f4aaee2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feat_cols = [col for col in hcp_feats_master_w_target_pdf.columns if col not in ['BH_ID', 'COHORT_MONTH', 'JIVI_NEW_WRITER_FLG']]\n",
    "binary_cols = ['AFFL_WI_INSN', 'AFFL_WI_JIVI_HCP_12M']\n",
    "numeric_cols = [col for col in feat_cols if col not in binary_cols]\n",
    "target_col_nm = 'JIVI_NEW_WRITER_FLG'\n",
    "print(\"Names of binary feats\", binary_cols)\n",
    "print(\"Names of numeric feats\", numeric_cols)\n",
    "print(\"Number of features: \", len(feat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a0f8c3-ed1b-4176-9b27-f7e71476c813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    feature_cols: List[str],\n",
    "    numeric_cols: List[str],\n",
    "    train_end_month: str,\n",
    "    scale: bool = False\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Prepare data for training and testing based on COHORT_MONTH.\n",
    "    \n",
    "    Args:\n",
    "        df: Input Pandas DataFrame\n",
    "        target_col: Name of target column\n",
    "        feature_cols: List of feature column names\n",
    "        train_end_month: End month for training data (YYYY-MM format)\n",
    "        scale: Whether to apply StandardScaler to the features\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test as Pandas DataFrames/Series\n",
    "    \"\"\"\n",
    "    # Ensure input is a pandas DataFrame\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "    \n",
    "    # Split data into train and test\n",
    "    train_mask = pd.to_datetime(df['COHORT_MONTH']).dt.strftime('%Y-%m') <= train_end_month\n",
    "    \n",
    "    # Create train/test splits using pandas\n",
    "    X_train = df[train_mask][feature_cols]\n",
    "    X_test = df[~train_mask][feature_cols]\n",
    "    y_train = df[train_mask][target_col]\n",
    "    y_test = df[~train_mask][target_col]\n",
    "\n",
    "    print(\"No. of features in input dataframe: \", len(feature_cols))\n",
    "    print(\"Positives/Negatives in train: \\n\", y_train.value_counts())\n",
    "    print(\"Positives/Negatives in test: \\n\", y_test.value_counts())\n",
    "    print(\"Shape of X_train: \", X_train.shape)\n",
    "    print(\"Shape of X_test: \", X_test.shape)\n",
    "    \n",
    "    # Scale features if scale is True\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train[numeric_cols] = pd.DataFrame(\n",
    "            scaler.fit_transform(X_train[numeric_cols]),\n",
    "            columns=numeric_cols,\n",
    "            index=X_train.index\n",
    "        )\n",
    "        X_test[numeric_cols] = pd.DataFrame(\n",
    "            scaler.transform(X_test[numeric_cols]),\n",
    "            columns=numeric_cols,\n",
    "            index=X_test.index\n",
    "        )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e147b3f-bcc1-4dcb-aa84-6d9dfe2c415f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# X = hcp_feats_master_w_target_pdf[feat_cols]\n",
    "X = hcp_feats_master_w_target_pdf[[\"COHORT_MONTH\"] + feat_cols]\n",
    "y = hcp_feats_master_w_target_pdf[target_col_nm]\n",
    "print(\"Positives/Negatives in the dataset: \\n\", y.value_counts())\n",
    "print(\"Shape of dataset before oversampling: \", hcp_feats_master_w_target_pdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "662b557e-5f56-4c1a-a4d5-49de45fc1cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Applying Oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9c587e-93a4-445e-b7fa-50da2fc85571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# applying oversampling for the minority class\n",
    "ros = RandomOverSampler()\n",
    "X_oversampled, y_oversampled = ros.fit_resample(X, y)\n",
    "hcp_feats_master_w_target_oversampled_pdf = pd.concat([X_oversampled, y_oversampled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb21811-f8f4-4016-9a54-f2defac34526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Positives/Negatives in dataset after oversampling: \\n\", hcp_feats_master_w_target_oversampled_pdf.JIVI_NEW_WRITER_FLG.value_counts())\n",
    "print(\"Shape of dataset after oversampling: \", hcp_feats_master_w_target_oversampled_pdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dad9de4e-a653-4b7b-bd90-42d590276127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Applying undersampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04a807d2-838e-4d90-96b7-6482d8760374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# applying undersampling for the majority class\n",
    "rus = RandomUnderSampler()\n",
    "X_undersampled, y_undersampled = rus.fit_resample(X, y)\n",
    "hcp_feats_master_w_target_undersampled_pdf = pd.concat([X_undersampled, y_undersampled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f360ab-540d-407e-b7ac-3c0a1bfb68c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Positives/Negatives in dataset after undersampling: \\n\", hcp_feats_master_w_target_undersampled_pdf.JIVI_NEW_WRITER_FLG.value_counts())\n",
    "print(\"Shape of dataset after undersampling: \", hcp_feats_master_w_target_undersampled_pdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5235b65-d628-4443-8c3b-3d4da438175e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Overall logistic regression performs consistently without overfitting and undersampling seems to be working better than oversampling for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00f83ce-07d1-4869-a6a6-581a39331ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create train and test dataset\n",
    "X_train, X_test, y_train, y_test = prepare_data(\n",
    "  hcp_feats_master_w_target_undersampled_pdf, \n",
    "  target_col_nm, \n",
    "  feat_cols,\n",
    "  numeric_cols,\n",
    "  train_end_month, \n",
    "  scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae68352-2be6-489a-a469-0834ab5e75c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.autolog()\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "\n",
    "# For Ridge regression\n",
    "# logit_reg = LogisticRegression(penalty='l2', class_weight='balanced', random_state=42, max_iter=1000)\n",
    "\n",
    "# For Lasso regression\n",
    "logit_reg = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=42, max_iter=1000)\n",
    "\n",
    "# Fit the model on the training data\n",
    "logit_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = logit_reg.predict(X_test)\n",
    "y_pred_proba = logit_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'auc_roc': roc_auc_score(y_test, y_pred_proba),\n",
    "    'precision': precision_score(y_test, y_pred),\n",
    "    'recall': recall_score(y_test, y_pred),\n",
    "    'f1': f1_score(y_test, y_pred),\n",
    "}\n",
    "\n",
    "# Calculate PR AUC\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "metrics['auc_pr'] = auc(recall, precision)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "metrics['TNs'] = cm[0, 0]\n",
    "metrics['FPs'] = cm[0, 1]\n",
    "metrics['FNs'] = cm[1, 0]\n",
    "metrics['TPs'] = cm[1, 1]\n",
    "\n",
    "\n",
    "for metric_name, value in metrics.items():\n",
    "  print(f\"{metric_name}: {value:.3f}\")\n",
    "  \n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "if y_pred_proba is not None:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(y_pred_proba, bins=50)\n",
    "    plt.title('Prediction Probability Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c84327-c461-41f7-ada6-2a2e4f8c430a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Get model co_efficients\n",
    "# co_eff = logit_reg.coef_[0]\n",
    "\n",
    "# # Put in DataFrame and sort by effect size\n",
    "# co_eff_df = pd.DataFrame()\n",
    "# co_eff_df['feature'] = feat_cols\n",
    "# co_eff_df['co_eff'] = co_eff\n",
    "# co_eff_df['abs_co_eff'] = np.abs(co_eff)\n",
    "# co_eff_df_sorted = co_eff_df.sort_values(by='abs_co_eff', ascending=False, inplace=False)\n",
    "# display(co_eff_df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83079f71-24a1-473f-ab6e-801f333c12f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SHAP feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cccc4b-c054-41e9-ac0b-37e403e25c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the SHAP explainer\n",
    "explainer = shap.Explainer(logit_reg, X_train)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(X_test)\n",
    "# shap_values = explainer(X_train)\n",
    "\n",
    "# Plot the SHAP summary plot\n",
    "# shap.summary_plot(shap_values, X_test, feature_names=feat_cols)\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca2a88d-5c7e-425a-9e73-af168e8cce1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values = explainer(X_train), \n",
    "                  features = X_train.values,\n",
    "                  feature_names = X_train.columns.values,\n",
    "                  plot_type='bar',\n",
    "                  max_display=15,\n",
    "                  show=False)\n",
    "plt.tight_layout(rect=[0, 0, 2, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2408426e-a411-49f9-9338-7d6efa57f020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize SHAP JavaScript visualization\n",
    "shap.initjs()\n",
    "\n",
    "# Select an index for the SHAP force plot\n",
    "ind = 1\n",
    "\n",
    "# Plot the SHAP force plot\n",
    "shap.force_plot(shap_values[ind], matplotlib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61d5ff03-f04f-4c72-800f-d77390bdb448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# top 20 features to show importance\n",
    "max_display = 20\n",
    "\n",
    "# For linear models, use coefficients directly\n",
    "importance = np.abs(logit_reg.coef_[0])\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feat_cols,\n",
    "    'importance': importance\n",
    "})\n",
    "feature_importance_df = feature_importance_df.sort_values(\n",
    "    'importance', ascending=False\n",
    ").head(max_display)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(\n",
    "    range(len(feature_importance_df)),\n",
    "    feature_importance_df['importance']\n",
    ")\n",
    "plt.yticks(\n",
    "    range(len(feature_importance_df)),\n",
    "    feature_importance_df['feature']\n",
    ")\n",
    "plt.xlabel('|Coefficient|')\n",
    "plt.title('Feature Importance (Logistic Regression Coefficients)')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "(Clone) 01_training_model_jivi_writers_explore",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
