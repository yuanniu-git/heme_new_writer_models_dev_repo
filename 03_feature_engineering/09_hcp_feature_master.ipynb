{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afbccf2e-50d5-44d0-b80a-0d4f2d55233c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# This notebook combines all the individual feature sets to the main HCP target spine dataset to create a master table containing all the features. \n",
    "### This dataset will go to machine learning model input as it will contain HCPs with their corresponding target class flag and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c8027d8-1de3-49f8-ade6-d3896e661b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "from pyspark.sql import functions as F  # Importing functions from pyspark.sql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b71c7453-ab18-435b-b3b8-4e347bf31bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../00_config/set-up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1ebdd7-8765-496c-9158-84261acbaeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Month and Date parameters for manual control\n",
    "first_month = \"2019-12\"\n",
    "last_month = \"2024-11\"\n",
    "\n",
    "study_period_start_date = \"2023-01-01\"\n",
    "study_period_start_month = \"2023-01\"\n",
    "study_period_end_date = \"2024-11-30\"\n",
    "study_period_end_month = \"2024-11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "253bf580-4743-4637-93de-27173d124686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_rows_with_nulls(df):\n",
    "    \"\"\"\n",
    "    Filters rows in the given DataFrame where any column contains a null value.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The input Spark DataFrame to filter.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame containing only the rows where at least one column is null.\n",
    "    \"\"\"\n",
    "    # Create a filter condition for rows where any column is null\n",
    "    filter_condition = None\n",
    "    for c in df.columns:\n",
    "        if filter_condition is None:\n",
    "            filter_condition = F.col(c).isNull()\n",
    "        else:\n",
    "            filter_condition |= F.col(c).isNull()\n",
    "    \n",
    "    # Apply the filter condition to the DataFrame\n",
    "    rows_with_nulls = df.filter(filter_condition)\n",
    "    return rows_with_nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671d4f58-be51-4b7c-9b9b-1d4ad0f2eece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading in all the needed (components) feature sets to create a master table containing features and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96018bd6-7a97-473b-bff0-a3d80526cc87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the calls activity data from Hivestore\n",
    "monthly_hcp_calls_feats_sdf = spark.sql(\"SELECT * FROM jivi_new_writer_model.monthly_hcp_calls_feats\")\n",
    "print(\n",
    "    \"Row count: \",\n",
    "    monthly_hcp_calls_feats_sdf.count(),\n",
    "    \"Column Count: \",\n",
    "    len(monthly_hcp_calls_feats_sdf.columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7d438a-ce2d-4dd1-aa17-aada788b91a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(monthly_hcp_calls_feats_sdf.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f6aa6b7-5766-4885-8f1d-c784e827d4c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the HCP monthly target spine from Hivestore\n",
    "hcp_target_spine_sdf = spark.sql(\"SELECT * FROM jivi_new_writer_model.hcp_target_spine\")\n",
    "print(\n",
    "    \"Row count: \",\n",
    "    hcp_target_spine_sdf.count(),\n",
    "    \"Column Count: \",\n",
    "    len(hcp_target_spine_sdf.columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1176788a-83ed-4796-b776-dd6d3be0f825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(hcp_target_spine_sdf.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f79cf3cb-ad29-4a4e-87ab-9866d44260e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the HCP monthly features from Hivestore\n",
    "hcp_monthly_feats_all_sdf = spark.sql(\"SELECT * FROM jivi_new_writer_model.all_hcp_monthly_features\")\n",
    "print(\n",
    "    \"Row count: \",\n",
    "    hcp_monthly_feats_all_sdf.count(),\n",
    "    \"Column Count: \",\n",
    "    len(hcp_monthly_feats_all_sdf.columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd4eae0-37a2-487b-9b14-6e38e2460223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(hcp_monthly_feats_all_sdf.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b79268e-78b3-4e61-b092-19566ec9903f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the count of null values for each column in the hcp_monthly_feats_all_sdf DataFrame\n",
    "display(hcp_monthly_feats_all_sdf.select([count(when(col(c).isNull(), c)).alias(c) for c in hcp_monthly_feats_all_sdf.columns]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2ab501b-72bf-4a24-beed-450a8b25c159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Joining the call activity feature set to the target spine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46832ae-fcae-4ea7-a303-622bb68bf5f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# changing the column names in the calls table to make the join easier between tables\n",
    "monthly_hcp_calls_feats_sdf = monthly_hcp_calls_feats_sdf.withColumnRenamed(\"BAYER_HCP_ID\", \"BH_ID\") \\\n",
    "                                                         .withColumnRenamed(\"CALL_MONTH\", \"COHORT_MONTH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fa4cc5-2314-40b7-af7a-d6502ee4133a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Join the HCP target spine Spark DataFrame with the monthly HCP calls features Spark DataFrame on BH_ID == BAYER_HCP_ID and COHORT_MONTH == CALL_MONTH using a left join\"\"\"\n",
    "\n",
    "hcp_monthly_calls_feats_with_target_sdf = hcp_target_spine_sdf.join(\n",
    "    monthly_hcp_calls_feats_sdf,\n",
    "    on=[\"BH_ID\", \"COHORT_MONTH\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "hcp_monthly_calls_feats_with_target_sdf = hcp_monthly_calls_feats_with_target_sdf.orderBy(F.desc(\"JIVI_NEW_WRITER_FLG\"), \"BH_ID\", \"COHORT_MONTH\")\n",
    "\n",
    "print(\n",
    "    \"Row count: \",\n",
    "    hcp_monthly_calls_feats_with_target_sdf.count(),\n",
    "    \"Column count: \",\n",
    "    len(hcp_monthly_calls_feats_with_target_sdf.columns),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4660e8d-7b20-489d-8d36-07c40543d8ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert hcp_target_spine_sdf.count() == hcp_monthly_calls_feats_with_target_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e0fbb5-6b98-4cf5-b7e3-e513062f1c42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(hcp_monthly_calls_feats_with_target_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a894b5ee-671a-4b31-8fe7-8c82f3910d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the function with your DataFrame\n",
    "rows_with_nulls = filter_rows_with_nulls(hcp_monthly_calls_feats_with_target_sdf)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(rows_with_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5aa51e1-8f3c-4d7c-b30f-5b938a3baa5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in nulls with zeros because for many HCPs in the target spine dataset, which is created from Overlap data, there may not be any calls or even they might not be on the call plan\"\"\"\n",
    "hcp_monthly_calls_feats_with_target_sdf = hcp_monthly_calls_feats_with_target_sdf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ddb1154-9321-460c-b82b-4223fc94dab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# checking the target class distribution in the features table\n",
    "display(hcp_monthly_calls_feats_with_target_sdf.groupBy('JIVI_NEW_WRITER_FLG').agg(F.countDistinct('BH_ID').alias('distinct_BH_ID_cnt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6bcab3c-8a1d-4120-9ac6-0fe33ed50731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Joining HCP features set to the target spine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c271f14-678e-4b87-8b65-b91929c2919a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Join the HCP target spine Spark DataFrame with the monthly HCP calls features Spark DataFrame on BH_ID == BAYER_HCP_ID and COHORT_MONTH == CALL_MONTH using a left join\"\"\"\n",
    "\n",
    "hcp_monthly_feats_with_target_sdf = hcp_monthly_calls_feats_with_target_sdf.join(\n",
    "    hcp_monthly_feats_all_sdf,\n",
    "    on=[\"BH_ID\", \"COHORT_MONTH\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "hcp_monthly_feats_with_target_sdf = hcp_monthly_feats_with_target_sdf.orderBy(F.desc(\"JIVI_NEW_WRITER_FLG\"), \"BH_ID\", \"COHORT_MONTH\")\n",
    "\n",
    "print(\n",
    "    \"Row count: \",\n",
    "    hcp_monthly_feats_with_target_sdf.count(),\n",
    "    \"Column count: \",\n",
    "    len(hcp_monthly_feats_with_target_sdf.columns),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "09_hcp_feature_master",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
